{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8d66687",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import needed libraries\n",
    "\n",
    "import torch as th\n",
    "import numpy as np\n",
    "from random import randint\n",
    "from random import choice\n",
    "from random import uniform\n",
    "from random import shuffle\n",
    "from scipy.stats import multivariate_normal\n",
    "from scipy.special import comb\n",
    "from scipy.special import binom\n",
    "from math import floor\n",
    "from math import exp\n",
    "import csv\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.autograd as autograd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from numba import jit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c8e124c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the graphlet equivalence class data from MATLAB\n",
    "\n",
    "matlab_data_3 = scipy.io.loadmat('classes3.mat')\n",
    "matlab_data_4 = scipy.io.loadmat('classes4.mat')\n",
    "matlab_data_5 = scipy.io.loadmat('classes5.mat')\n",
    "matlab_data_6 = scipy.io.loadmat('classes6.mat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "298528fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# These identify which equivalence class each graph belongs to (enumerated by binary counting from 0 to n*(n-1)/2 -1)\n",
    "classes3 = matlab_data_3['classes_master']\n",
    "classes4 = matlab_data_4['classes_master']\n",
    "classes5 = matlab_data_5['classes_master']\n",
    "classes6 = matlab_data_6['classes_master']\n",
    "\n",
    "# These are a list of an example graph belonging to each equivalence class\n",
    "graphs3 = matlab_data_3['all_graphs']\n",
    "graphs4 = matlab_data_4['all_graphs']\n",
    "graphs5 = matlab_data_5['all_graphs']\n",
    "graphs6 = matlab_data_6['all_graphs']\n",
    "\n",
    "# These are the reverse of classesX. They list the graphs that belong to each equivalence class\n",
    "reverse3 = matlab_data_3['reverse_master']\n",
    "reverse4 = matlab_data_4['reverse_master']\n",
    "reverse5 = matlab_data_5['reverse_master']\n",
    "reverse6 = matlab_data_6['reverse_master']\n",
    "\n",
    "# Total number of graphlet equivalence classes of each size. \n",
    "num_classes_3 = graphs3.shape[2]\n",
    "num_classes_4 = graphs4.shape[2]\n",
    "num_classes_5 = graphs5.shape[2]\n",
    "num_classes_6 = graphs6.shape[2]\n",
    "#num_classes_high = graphs_high.shape[2]\n",
    "num_classes_total = num_classes_3 + num_classes_4 + num_classes_5 + num_classes_6\n",
    "#num_classes_total = num_classes_3 + num_classes_4 + num_classes_5 + num_classes_6 + num_classes_high\n",
    "\n",
    "# = 4 + 11 + 34 + 156 + X = 205 + X\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1acc4dce",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_25988/399665653.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# This section creates the neural network that does all the work in this problem\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mclass\u001b[0m \u001b[0mgraph_learner\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mHbars\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_nodes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_features\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkern\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkern_deriv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmulti\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmulti_probs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_nodes_multi\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdropout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgraph_learner\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'nn' is not defined"
     ]
    }
   ],
   "source": [
    "# This section creates the neural network that does all the work in this problem\n",
    "\n",
    "class graph_learner(nn.Module):\n",
    "    def __init__(self, Hbars, max_nodes, num_features, kern, kern_deriv, multi=0, multi_probs=[], max_nodes_multi=0, dropout=0):\n",
    "        super(graph_learner, self).__init__()\n",
    "        \n",
    "        # input parameter Hbars is empirical proportions of graphlets as a vector 205 x 1. First 4 entries are the 3-graphlets, next 11 are the 4-graphlets...\n",
    "        # max_nodes is maximum number of nodes\n",
    "        # num_features is the dimension of the vector space in which we create the embeddings\n",
    "        \n",
    "        # set up neural network architecture\n",
    "        self.input_size = 30 # dimension of normal random variable for sampling\n",
    "        self.max_nodes = max_nodes # max number of nodes to create. Currently it always creates the maximum number of nodes.\n",
    "        self.num_features = num_features\n",
    "        self.output_size = max_nodes * (num_features+2)\n",
    "        self.hidden_size1 = 10 # number of neurons at hidden layers\n",
    "        self.hidden_size2 = 10\n",
    "        self.num_samples = 10\n",
    "        self.Hbar = Hbars # target proportions \n",
    "        self.kern = kern\n",
    "        self.kern_deriv = kern_deriv\n",
    "        self.multi = multi\n",
    "        self.multi_probs = multi_probs\n",
    "        self.max_nodes_multi = max_nodes_multi\n",
    "        self.dropout = dropout\n",
    "        \n",
    "        # create random starting weights\n",
    "        self.W1 = th.randn(self.input_size, self.hidden_size1, requires_grad=True)\n",
    "        self.W2 = th.randn(self.hidden_size1, self.hidden_size2, requires_grad=True)\n",
    "        self.W3 = th.randn(self.hidden_size2, self.output_size, requires_grad=True)\n",
    "        \n",
    "        #####################\n",
    "        self.f1 = th.randn(self.input_size)\n",
    "        self.f2 = th.randn(self.input_size)\n",
    "        #####################\n",
    "        \n",
    "    def forward(self, X):        \n",
    "        # forward pass \n",
    "        # leaky relu used instead of regular relu because zero embeddings cause divide by 0 errors\n",
    "        self.z1 = F.leaky_relu(th.matmul(X, self.W1)) \n",
    "        self.z2 = F.leaky_relu(th.matmul(self.z1, self.W2)) \n",
    "        self.z3 = th.matmul(self.z2, self.W3)\n",
    "        return self.z3\n",
    "    \n",
    "        \n",
    "    def train(self, minibatch, gamma, display, bipart, max_order, order_probs, limited_graphlets=0, usable_graphlets3=None, usable_graphlets4=None, usable_graphlets5=None, usable_graphlets6=None,):\n",
    "        # minibatch is number of samples to take before doing an update\n",
    "        # gamma is learning rate\n",
    "        # activate display to put some info onscreen during training\n",
    "        # order probs is a vector of probabilities for each order of graphlet to be selected\n",
    "        # bipart is deprecated\n",
    "        \n",
    "        if not self.multi:\n",
    "            sample_nodes1_all = self.better_sampler(self.max_nodes, max_order, minibatch)\n",
    "            sample_nodes2_all = self.better_sampler(self.max_nodes, max_order, minibatch)\n",
    "        \n",
    "        update_W1 = 0\n",
    "        update_W2 = 0\n",
    "        update_W3 = 0\n",
    "        \n",
    "        for i in range(minibatch):\n",
    "            # perform a forward pass on two samples of random noise\n",
    "            sample1_out = self.forward(th.randn(self.input_size))\n",
    "            sample2_out = self.forward(th.randn(self.input_size))\n",
    "            sample1_reshape = sample1_out.view(self.max_nodes,-1).detach().numpy()\n",
    "            sample2_reshape = sample2_out.view(self.max_nodes,-1).detach().numpy()\n",
    "            \n",
    "            # error check\n",
    "            if th.equal(sample1_out, th.zeros(self.max_nodes*self.num_features)) or th.equal(sample2_out, th.zeros(self.max_nodes*self.num_features)):\n",
    "                print(\"Error: Samples identical\")\n",
    "                print(sample1_out)\n",
    "                print(sample2_out)\n",
    "                print(self.W1,self.W2,self.W3)\n",
    "            \n",
    "            # sample a subset of the nodes and an equivalence class of graphlet uniformly\n",
    "            \n",
    "            # this line controls how often we sample graphlets of each size. \n",
    "            size_sample = np.random.choice([3,4,5,6,7], p=order_probs)\n",
    "\n",
    "            if size_sample == 3:\n",
    "                if not self.multi:\n",
    "                    sample_nodes1 = sample_nodes1_all[i,0:3]\n",
    "                    sample_nodes2 = sample_nodes2_all[i,0:3]\n",
    "                # select which graphlet (equivalence class) to work with. \n",
    "                if limited_graphlets:\n",
    "                    idx = floor(len(useable_graphlets3)*np.random.rand())\n",
    "                    sample_graphlet_ID1 = usable_graphlets3[idx] \n",
    "                else:\n",
    "                    sample_graphlet_ID1 = floor(num_classes_3*np.random.rand()) \n",
    "                overall_ID = sample_graphlet_ID1\n",
    "                for k in range(reverse3.shape[1]):\n",
    "                    if reverse3[sample_graphlet_ID1,k] == -1: # exploit the data structure to determine how many graphs are in this equivalence class\n",
    "                        rho1 = k # this is k-1+1\n",
    "                        if rho1 == 0:\n",
    "                            print(\"Error: calculation of rho\")\n",
    "                        break\n",
    "                sample_L1 = self.int2graph(reverse3[sample_graphlet_ID1,np.random.randint(rho1)],3) # choose a random graph in the equivalence class. In the document, this is the selection of L.\n",
    "                sample_L2 = self.int2graph(reverse3[sample_graphlet_ID1,np.random.randint(rho1)],3) # choose another.\n",
    "                Hbar_L1 = self.Hbar[sample_graphlet_ID1] # Get the target proportion of this graphlet\n",
    "            elif size_sample == 4:\n",
    "                if not self.multi:\n",
    "                    sample_nodes1 = sample_nodes1_all[i,0:4]\n",
    "                    sample_nodes2 = sample_nodes2_all[i,0:4]\n",
    "                if limited_graphlets:\n",
    "                    idx = floor(len(useable_graphlets4)*np.random.rand())\n",
    "                    sample_graphlet_ID1 = usable_graphlets4[idx] \n",
    "                else:\n",
    "                    sample_graphlet_ID1 = floor(num_classes_4*np.random.rand())\n",
    "                overall_ID = sample_graphlet_ID1+num_classes_3\n",
    "                for k in range(reverse4.shape[1]):\n",
    "                    if reverse4[sample_graphlet_ID1,k] == -1:\n",
    "                        rho1 = k # this is k-1+1\n",
    "                        if rho1 == 0:\n",
    "                            print(\"Error: calculation of rho\")\n",
    "                        break\n",
    "                sample_L1 = self.int2graph(reverse4[sample_graphlet_ID1,np.random.randint(rho1)],4)\n",
    "                sample_L2 = self.int2graph(reverse4[sample_graphlet_ID1,np.random.randint(rho1)],4)\n",
    "                Hbar_L1 = self.Hbar[overall_ID]\n",
    "            elif size_sample == 5:\n",
    "                if not self.multi:\n",
    "                    sample_nodes1 = sample_nodes1_all[i,0:5]\n",
    "                    sample_nodes2 = sample_nodes2_all[i,0:5]\n",
    "                if limited_graphlets:\n",
    "                    idx = floor(len(useable_graphlets5)*np.random.rand())\n",
    "                    sample_graphlet_ID1 = usable_graphlets5[idx] \n",
    "                else:\n",
    "                    sample_graphlet_ID1 = floor(num_classes_5*np.random.rand()) \n",
    "                overall_ID = sample_graphlet_ID1+num_classes_3+num_classes_4\n",
    "                #sample_graphlet1 = graphs5[:,:,sample_graphlet_ID1]\n",
    "                for k in range(reverse5.shape[1]):\n",
    "                    if reverse5[sample_graphlet_ID1,k] == -1:\n",
    "                        rho1 = k # this is k-1+1\n",
    "                        if rho1 == 0:\n",
    "                            print(\"Error: calculation of rho\")\n",
    "                        break\n",
    "                sample_L1 = self.int2graph(reverse5[sample_graphlet_ID1,np.random.randint(rho1)],5)\n",
    "                sample_L2 = self.int2graph(reverse5[sample_graphlet_ID1,np.random.randint(rho1)],5)\n",
    "                Hbar_L1 = self.Hbar[overall_ID]\n",
    "            elif size_sample == 6:\n",
    "                if not self.multi:\n",
    "                    sample_nodes1 = sample_nodes1_all[i,0:6]\n",
    "                    sample_nodes2 = sample_nodes2_all[i,0:6]\n",
    "                if limited_graphlets:\n",
    "                    idx = floor(len(useable_graphlets6)*np.random.rand())\n",
    "                    sample_graphlet_ID1 = usable_graphlets6[idx]  \n",
    "                else:\n",
    "                    sample_graphlet_ID1 = floor(num_classes_6*np.random.rand()) \n",
    "                overall_ID = sample_graphlet_ID1+num_classes_3+num_classes_4+num_classes_5\n",
    "                #sample_graphlet1 = graphs6[:,:,sample_graphlet_ID1]\n",
    "                for k in range(reverse6.shape[1]):\n",
    "                    if reverse6[sample_graphlet_ID1,k] == -1:\n",
    "                        rho1 = k # this is k-1+1\n",
    "                        if rho1 == 0:\n",
    "                            print(\"Error: calculation of rho\")\n",
    "                        break\n",
    "                sample_L1 = self.int2graph(reverse6[sample_graphlet_ID1,np.random.randint(rho1)],6)\n",
    "                sample_L2 = self.int2graph(reverse6[sample_graphlet_ID1,np.random.randint(rho1)],6)\n",
    "                Hbar_L1 = self.Hbar[overall_ID]\n",
    "            elif size_sample == 7:\n",
    "                #TODO: Complete this section (should not be used currently)\n",
    "                sample_nodes1 = sample_nodes1_all[i,0:7]\n",
    "                sample_nodes2 = sample_nodes2_all[i,0:7]\n",
    "                sample_graphlet_ID1 = floor(num_classes_high*np.random.rand()) \n",
    "                overall_ID = sample_graphlet_ID1+num_classes_3+num_classes_4+num_classes_5+num_classes_6\n",
    "                for k in range(reverse_high.shape[1]):\n",
    "                    if reverse_high[sample_graphlet_ID1,k] == -1:\n",
    "                        rho1 = k # this is k-1+1\n",
    "                        if rho1 == 0:\n",
    "                            print(\"Error: calculation of rho\")\n",
    "                        break\n",
    "                sample_L1 = self.int2graph(reverse_high[sample_graphlet_ID1,np.random.randint(rho1)],7)\n",
    "                sample_L2 = self.int2graph(reverse_high[sample_graphlet_ID1,np.random.randint(rho1)],7)\n",
    "                Hbar_L1 = self.Hbar[overall_ID]\n",
    "\n",
    "            # part of the chain rule\n",
    "            if self.multi:\n",
    "                prob1 = self.PLA_noA(sample_L1, sample1_reshape, size_sample, self.dropout)\n",
    "                prob2 = self.PLA_noA(sample_L2, sample2_reshape, size_sample, self.dropout)\n",
    "                prob_prime1_z = self.PLA_prime_noA(sample_L1, sample1_reshape, size_sample, self.dropout)\n",
    "                prob_prime2_z = self.PLA_prime_noA(sample_L2, sample2_reshape, size_sample, self.dropout)\n",
    "            else:\n",
    "                prob1 = self.PLA(sample_L1, sample1_reshape, sample_nodes1, size_sample, self.dropout) # Use below function to calculate prob of vertex set A having edge set L\n",
    "                prob2 = self.PLA(sample_L2, sample2_reshape, sample_nodes2, size_sample, self.dropout)\n",
    "                prob_prime1_z = self.PLA_prime(sample_L1, sample1_reshape, sample_nodes1, size_sample, self.dropout) # 1 x size_sample * num_features\n",
    "                prob_prime2_z = self.PLA_prime(sample_L2, sample2_reshape, sample_nodes2, size_sample, self.dropout)\n",
    "\n",
    "            # make it a tensor\n",
    "            y1 = th.from_numpy(prob_prime1_z)\n",
    "            y2 = th.from_numpy(prob_prime2_z)\n",
    "            y1 = y1.type(th.FloatTensor)\n",
    "            y2 = y2.type(th.FloatTensor)\n",
    "\n",
    "            # Perform backprop to get gradients for weights\n",
    "            sample1_out.backward(y1, retain_graph=False)\n",
    "            W1_update_i1 = self.W1.grad.clone().detach()\n",
    "            W2_update_i1 = self.W2.grad.clone().detach()\n",
    "            W3_update_i1 = self.W3.grad.clone().detach()\n",
    "            \n",
    "            # zero out the gradients after storing them above\n",
    "            self.W1.grad.data.zero_()\n",
    "            self.W2.grad.data.zero_()\n",
    "            self.W3.grad.data.zero_()\n",
    "            \n",
    "            # same thing, other sample\n",
    "            sample2_out.backward(y2, retain_graph=False)\n",
    "            W1_update_i2 = self.W1.grad.clone().detach()\n",
    "            W2_update_i2 = self.W2.grad.clone().detach()\n",
    "            W3_update_i2 = self.W3.grad.clone().detach()\n",
    "            self.W1.grad.data.zero_()\n",
    "            self.W2.grad.data.zero_()\n",
    "            self.W3.grad.data.zero_()\n",
    "\n",
    "            # These three lines are critical\n",
    "            update_W1 += -gamma * (rho1*rho1) * ( W1_update_i1 * (prob2 - (1/rho1)*Hbar_L1) + W1_update_i2 * (prob1 - (1/rho1)*Hbar_L1))\n",
    "            update_W2 += -gamma * (rho1*rho1) * ( W2_update_i1 * (prob2 - (1/rho1)*Hbar_L1) + W2_update_i2 * (prob1 - (1/rho1)*Hbar_L1))\n",
    "            update_W3 += -gamma * (rho1*rho1) * ( W3_update_i1 * (prob2 - (1/rho1)*Hbar_L1) + W3_update_i2 * (prob1 - (1/rho1)*Hbar_L1))\n",
    "            \n",
    "        # update weights\n",
    "        with th.no_grad():\n",
    "            self.W1 += update_W1\n",
    "            self.W2 += update_W2\n",
    "            self.W3 += update_W3\n",
    "                        \n",
    "        if display:\n",
    "            #print(self.W1) # activate to display weights\n",
    "            #print(update_W1) # activate to display update to weights\n",
    "            sample_size = 100\n",
    "            sample_graphlets = np.zeros([self.Hbar.size,sample_size])\n",
    "            difference = np.zeros([self.Hbar.size,sample_size])\n",
    "            # To get an error we can display, we'll need to generate some graphs using the learned weights and calculate their graphlets\n",
    "            for i in range(sample_size): # Use 20 graphs to estimate our distribution\n",
    "                sample_graph = self.generate_graph()\n",
    "                sample_graphlets[:,i] = self.graph2graphlets(sample_graph, max_order, classes3, classes4, classes5, classes6)\n",
    "                difference[:,i] = sample_graphlets[:,i] - self.Hbar\n",
    "            difference_avg = np.mean(difference,axis=1)\n",
    "            sample_graphlets_avg = np.mean(sample_graphlets,axis=1)\n",
    "            if max_order == 3:\n",
    "                total_diff = np.sum(np.absolute(difference_avg[0:4]))\n",
    "                print(sample_graphlets_avg[0:4])\n",
    "                print(self.Hbar[0:4])\n",
    "                print(difference_avg[0:4])\n",
    "                print(total_diff)\n",
    "            elif max_order == 4:\n",
    "                total_diff = np.sum(np.absolute(difference_avg[0:15]))\n",
    "                print(sample_graphlets_avg[0:15])\n",
    "                print(self.Hbar[0:15])\n",
    "                print(difference_avg[0:15])\n",
    "                print(total_diff)\n",
    "            elif max_order == 5:\n",
    "                total_diff = np.sum(np.absolute(difference_avg[0:49]))\n",
    "                print(sample_graphlets_avg[0:49])\n",
    "                print(self.Hbar[0:49])\n",
    "                print(difference_avg[0:49])\n",
    "                print(total_diff)\n",
    "            else:\n",
    "                print(\"Error: this section of the code is incomplete\")         \n",
    "        else:\n",
    "            total_diff = 100\n",
    "        \n",
    "        return total_diff\n",
    "    \n",
    "    def generate_graph(self):\n",
    "        # generates an actual graph using the results from the neural net\n",
    "        num_nodes = self.max_nodes\n",
    "        adj = th.zeros(num_nodes,num_nodes) # adjacency matrix\n",
    "        \n",
    "        # perform a forward pass to get embeddings\n",
    "        with th.no_grad():\n",
    "            forward_pass_flat = self.forward(th.randn(self.input_size))\n",
    "            #forward_pass_flat = self.forward(th.randn(self.input_size)+100*th.ones(self.input_size))\n",
    "            #forward_pass_flat = self.forward(self.f1)\n",
    "            forward_pass = forward_pass_flat.view(self.max_nodes,self.num_features).detach()\n",
    "        \n",
    "        for i in range(num_nodes):\n",
    "            for j in range(i+1,num_nodes):\n",
    "                # calculate edge probs\n",
    "                p = self.kern(forward_pass[i,:],forward_pass[j,:])\n",
    "                # realize a graph from probs\n",
    "                rand = uniform(0, 1)\n",
    "                if rand < p:\n",
    "                    adj[i,j] = 1\n",
    "                    adj[j,i] = 1\n",
    "        return adj\n",
    "    \n",
    "\n",
    "    def hv(self,x):\n",
    "        # hv stands for heaviside even though this is not quite the heaviside function\n",
    "        # needed for derivative\n",
    "        if x >= 0:\n",
    "            return 1\n",
    "        else:\n",
    "            return -1\n",
    "        \n",
    "    def sigmoid(self, sigm):\n",
    "        return 1 / (1 + exp(-sigm))\n",
    "    \n",
    "    def sigm_deriv(self,sigm):\n",
    "        #if sigm > \n",
    "        return exp(-sigm)/(exp(-sigm)+1)**2\n",
    "    \n",
    "    def int2graph(self, I, n):\n",
    "        # converts an integer I to an n x n graph\n",
    "\n",
    "        binary = \"{0:b}\".format(I)[::-1]\n",
    "        \n",
    "        m = int(n*(n-1)/2)\n",
    "        num_zeros_to_add = m - len(binary)\n",
    "        zeros_to_add = \"0\"*num_zeros_to_add\n",
    "        binary = binary + zeros_to_add\n",
    "\n",
    "        graph = np.zeros([n,n])\n",
    "        count = 0\n",
    "        for j in range(1,n):\n",
    "            for i in range(j):\n",
    "                graph[i,j] = int(binary[count])\n",
    "                graph[j,i] = graph[i,j]\n",
    "                count += 1\n",
    "        return graph\n",
    "    \n",
    "    #@jit\n",
    "    def graph2int(self,adj):\n",
    "    # converts a graph whose adjacency matrix is adj into a number between 0 and 2^(n*(n-1)/2) - 1 using binary representation\n",
    "    # same function as above\n",
    "    \n",
    "        n = adj.shape[0]\n",
    "        bin_vector = np.array([adj[0,1]])\n",
    "\n",
    "        for i in range(2,n):\n",
    "            appendage = adj[0:i,i]\n",
    "            bin_vector = np.append(bin_vector, appendage)\n",
    "\n",
    "        out = 0\n",
    "        for i in range(bin_vector.size):\n",
    "            out += int(2**i * bin_vector[i])\n",
    "\n",
    "        return out\n",
    "        \n",
    "    #@jit\n",
    "    def graph2graphlets(self, adj, max_order, graphlet_list_3, graphlet_list_4, graphlet_list_5, graphlet_list_6, counts_or_prop=0):\n",
    "        # converts a graph given in adj to a list of graphlet proportions via sampling\n",
    "        \n",
    "        n = adj.shape[0]\n",
    "\n",
    "        graphlets_2 = np.zeros([2,1])\n",
    "        graphlets_3 = np.zeros([4,1])\n",
    "        graphlets_4 = np.zeros([11,1])\n",
    "        graphlets_5 = np.zeros([34,1])\n",
    "        graphlets_6 = np.zeros([156,1])\n",
    "\n",
    "\n",
    "        # Count the 2-graphlets ie. edges\n",
    "        #num_2 = n*(n-1)/2 \n",
    "        #graphlets_2[0] = (np.sum(adj)/2) / num_2 \n",
    "        #graphlets_2[1] = (num_2 - graphlets_2[0]) / num_2\n",
    "\n",
    "\n",
    "        # Count the 3-graphlets (via sampling)\n",
    "        num_3 = scipy.special.binom(n,3)\n",
    "        num_samples_3 = 100\n",
    "        \n",
    "        nodes_all = self.better_sampler(n, 3, num_samples_3)\n",
    "\n",
    "        for i in range(num_samples_3):\n",
    "            # randomly select 3 nodes from the list, check what graph they form, check its equivalence class, add to count\n",
    "            nodes = nodes_all[i,:]\n",
    "            sub_adj = adj[nodes[:, None], nodes]\n",
    "            sub_num = self.graph2int(sub_adj)\n",
    "            graphlet_ID = graphlet_list_3[sub_num]\n",
    "            graphlets_3[graphlet_ID] += 1\n",
    "\n",
    "        graphlets_3 = graphlets_3 / num_samples_3\n",
    "        if counts_or_prop:\n",
    "            graphlets_3 = graphlets_3 * num_3\n",
    "\n",
    "        if max_order >= 4:\n",
    "            # Count the 4-graphlets (via sampling)\n",
    "            num_4 = scipy.special.binom(n,4)\n",
    "            num_samples_4 = 300\n",
    "            nodes_all = self.better_sampler(n, 4, num_samples_4)\n",
    "\n",
    "            for i in range(num_samples_4):\n",
    "                # randomly select 3 nodes from the list\n",
    "                nodes = nodes_all[i,:]\n",
    "                sub_adj = adj[nodes[:, None], nodes]\n",
    "                sub_num = self.graph2int(sub_adj)\n",
    "                graphlet_ID = graphlet_list_4[sub_num]\n",
    "                graphlets_4[graphlet_ID] += 1\n",
    "\n",
    "            graphlets_4 = graphlets_4 / num_samples_4\n",
    "            if counts_or_prop:\n",
    "                graphlets_4 = graphlets_4 * num_4\n",
    "\n",
    "        if max_order >= 5:\n",
    "            # Count the 5-graphlets (via sampling)\n",
    "            num_5 = scipy.special.binom(n,5)\n",
    "            num_samples_5 = 900\n",
    "            nodes_all = self.better_sampler(n, 5, num_samples_5)\n",
    "\n",
    "            for i in range(num_samples_5):\n",
    "                # randomly select 5 nodes from the list\n",
    "                nodes = nodes_all[i,:]\n",
    "                sub_adj = adj[nodes[:, None], nodes]\n",
    "                sub_num = self.graph2int(sub_adj)\n",
    "                graphlet_ID = graphlet_list_5[sub_num]\n",
    "                graphlets_5[graphlet_ID] += 1\n",
    "\n",
    "            graphlets_5 = graphlets_5 / num_samples_5\n",
    "            if counts_or_prop:\n",
    "                graphlets_5 = graphlets_5 * num_5\n",
    "\n",
    "        if max_order >= 6:\n",
    "            # Count the 6-graphlets (via sampling)\n",
    "            num_6 = scipy.special.binom(n,6)\n",
    "            num_samples_6 = 4500\n",
    "            nodes_all = self.better_sampler(n, 6, num_samples_6)\n",
    "\n",
    "            for i in range(num_samples_6):\n",
    "                # randomly select 6 nodes from the list\n",
    "                nodes = nodes_all[i,:]\n",
    "                sub_adj = adj[nodes[:, None], nodes]\n",
    "                sub_num = self.graph2int(sub_adj)\n",
    "                graphlet_ID = graphlet_list_6[sub_num]\n",
    "                graphlets_6[graphlet_ID] += 1\n",
    "\n",
    "            graphlets_6 = graphlets_6 / num_samples_6\n",
    "            if counts_or_prop:\n",
    "                graphlets_6 = graphlets_6 * num_6\n",
    "    \n",
    "\n",
    "        # activate below line to use 2-graphlets also\n",
    "        #graphlets = np.concatenate((graphlets_2,graphlets_3,graphlets_4,graphlets_5,graphlets_6)).ravel() \n",
    "        graphlets = np.concatenate((graphlets_3,graphlets_4,graphlets_5,graphlets_6)).ravel()\n",
    "        return graphlets\n",
    "    \n",
    "    def better_sampler(self, maxi, length, num_samples):\n",
    "        y = np.arange(maxi)\n",
    "        x = np.zeros([num_samples,length])\n",
    "        for i in range(num_samples):\n",
    "            np.random.shuffle(y)\n",
    "            x[i,:] = y[0:length]\n",
    "        return x.astype(int)\n",
    "\n",
    "    def PLA(self, adj, embeddings, node_selections, n, drop=0):\n",
    "        # function to compute the probability that a particular set of nodes given in node_selections will form a particular subgraph given by adj\n",
    "        embeddings_subgraph = embeddings[node_selections,:] # assume each row is a vertex and full row is its embedding. This represents A_j in the doc\n",
    "\n",
    "        result = 1\n",
    "        for i in range(n):\n",
    "            for j in range(i+1,n):\n",
    "                if adj[i,j] == 1: # multiply by edge prob\n",
    "                    #if np.linalg.norm(embeddings_subgraph[i,:]) == 0:\n",
    "                    #    print(\"Error: Norm still too small\")\n",
    "                    result = result*self.kern(embeddings_subgraph[i,2:], embeddings_subgraph[j,2:])\n",
    "                elif adj[i,j] == 0: # multiply by 1 - edge prob\n",
    "                    #if np.linalg.norm(embeddings_subgraph[i,:]) == 0:\n",
    "                    #    print(\"Error: Norm still too small\")\n",
    "                    result = result*(1-self.kern(embeddings_subgraph[i,2:], embeddings_subgraph[j,2:]))\n",
    "            if drop:\n",
    "                result = result*self.sigmoid(embeddings_subgraph[i,1])\n",
    "        return result\n",
    "    \n",
    "    def PLA_noA(self, adj, embeddings, n, drop=0):\n",
    "        # As above, but used for community model. Directly computes probability without sampling A. \n",
    "        num_comms = embeddings.shape[0]\n",
    "        comms = self.comm_list_maker2(num_comms,n)\n",
    "        total = 0\n",
    "                \n",
    "        for t in range(len(comms)):\n",
    "            result = 1\n",
    "            for i in range(n):\n",
    "                for j in range(i+1,n):\n",
    "                    if adj[i,j] == 1: # multiply by edge prob\n",
    "                        result = result*self.kern(embeddings[comms[t][i],2:], embeddings[comms[t][j],2:])\n",
    "                    elif adj[i,j] == 0: # multiply by 1 - edge prob\n",
    "                        result = result*(1-self.kern(embeddings[comms[t][i],2:], embeddings[comms[t][j],2:]))\n",
    "                if drop:\n",
    "                    result = result*self.sigmoid(embeddings[comms[t][i],1:])\n",
    "                        \n",
    "            comm_probs = embeddings[:,0]\n",
    "            s_max = scipy.special.softmax(comm_probs)\n",
    "            for i in range(len(comms[t])):\n",
    "                result = result*s_max[comms[t][i]]\n",
    "            total += result\n",
    "        return total\n",
    "    \n",
    "    def PLA_alt(self, adj, embeddings, node_selections, n, i_remove, j_remove, drop=0):\n",
    "        # alternate version of PLA needed to avoid divide by zero issue. Used only in derivative calculation.\n",
    "        embeddings_subgraph = embeddings[node_selections,:] \n",
    "        flag = 1\n",
    "        \n",
    "        result = 1\n",
    "        for i in range(n):\n",
    "            for j in range(i+1,n):\n",
    "                if (i == i_remove and j == j_remove) or (j == i_remove and i == j_remove):\n",
    "                    flag = 0\n",
    "                    continue\n",
    "                elif adj[i,j] == 1:\n",
    "                    result = result*self.kern(embeddings_subgraph[i,2:], embeddings_subgraph[j,2:])\n",
    "                elif adj[i,j] == 0:\n",
    "                    result = result*(1-self.kern(embeddings_subgraph[i,2:], embeddings_subgraph[j,2:]))\n",
    "            if drop:\n",
    "                result = result*self.sigmoid(embeddings_subgraph[i,1])\n",
    "        if flag == 1:\n",
    "            print(\"Error: ij not removed\")\n",
    "        return result\n",
    "    \n",
    "    def PLA_alt2(self, adj, embeddings, node_selections, n, i_remove):\n",
    "        # alternate version of PLA needed to avoid divide by zero issue. Used only in derivative calculation.\n",
    "        # this version is to remove the node removal probability\n",
    "        embeddings_subgraph = embeddings[node_selections,:] \n",
    "        flag = 1\n",
    "        \n",
    "        result = 1\n",
    "        for i in range(n):\n",
    "            for j in range(i+1,n):\n",
    "                if adj[i,j] == 1:\n",
    "                    result = result*self.kern(embeddings_subgraph[i,2:], embeddings_subgraph[j,2:])\n",
    "                elif adj[i,j] == 0:\n",
    "                    result = result*(1-self.kern(embeddings_subgraph[i,2:], embeddings_subgraph[j,2:]))\n",
    "                else:\n",
    "                    print(\"Error: Adjacency matrix incorrect\")\n",
    "            if i == i_remove:\n",
    "                flag = 0\n",
    "                continue\n",
    "            else:\n",
    "                result = result*self.sigmoid(embeddings_subgraph[i,1])\n",
    "            \n",
    "        if flag == 1:\n",
    "            print(\"Error: i not removed\")\n",
    "        return result\n",
    "    \n",
    "    def PLA_prime(self, adj, embeddings, node_selections, n, drop=0):\n",
    "    # output should be 1 x s*k\n",
    "        \n",
    "    # derivative of PLA wrt. weights\n",
    "    \n",
    "        embeddings_subgraph = embeddings[node_selections,:] # assume each row is a vertex and full row is its embedding\n",
    "        embeddings_length = embeddings.shape[1]\n",
    "\n",
    "        del_P_del_z = np.zeros([embeddings.shape[0],embeddings.shape[1]])\n",
    "        for i in range(n): \n",
    "            # for each node, compute derivative of P wrt that node's embedding, multiply by derivative of embedding wrt weights, add up\n",
    "            total_i = np.zeros([embeddings_length])\n",
    "            for j in range(n):\n",
    "                if i == j:\n",
    "                    continue\n",
    "\n",
    "                #########################\n",
    "                embed_i = embeddings_subgraph[i,2:]\n",
    "                embed_j = embeddings_subgraph[j,2:]\n",
    "                \n",
    "                #col vector\n",
    "                #for k in range(embeddings_length):\n",
    "                    #phi_ij_prime[k] = (self.hv(phi_ij)*embeddings_subgraph[j,k]*norm_i*norm_j - norm_j*embeddings_subgraph[i,k]*abs(phi_ij)/norm_i) / (norm_i**2 * norm_j**2) # calculated derivative\n",
    "                phi_ij_prime = self.kern_deriv(embed_i, embed_j)\n",
    "                #########################\n",
    "                \n",
    "                if adj[i,j] == 1:\n",
    "                    # chain rule\n",
    "                    total_i += phi_ij_prime * self.PLA_alt(adj,embeddings,node_selections,n,i,j,self.dropout)                   \n",
    "                elif adj[i,j] == 0:\n",
    "                    total_i += -phi_ij_prime * self.PLA_alt(adj,embeddings,node_selections,n,i,j,self.dropout)\n",
    "            \n",
    "            # derivatives wrt dropout probs\n",
    "            if drop:\n",
    "                # can apply a scale factor here or elsewhere if needed\n",
    "                removal_prob_deriv = self.PLA_alt2(adj,embeddings,node_selections,n,i) * self.sigm_deriv(embeddings_subgraph[i,1])\n",
    "                total_i = total_i.ravel()\n",
    "                total_i = np.insert(total_i, 0, [0,removal_prob_deriv])\n",
    "            else:\n",
    "                total_i = total_i.ravel()\n",
    "                total_i = np.insert(total_i, 0, [0,0])\n",
    "                    \n",
    "            del_P_del_z[node_selections[i],:] = total_i # put the derivatives in the correct place for each node\n",
    "             \n",
    "        del_P_del_z = del_P_del_z.ravel()\n",
    "        \n",
    "        return del_P_del_z\n",
    "    \n",
    "    def PLA_prime_noA(self, adj, embeddings, n, drop=0):\n",
    "    # output should be 1 x s*k\n",
    "    # derivative of PLA wrt. weights\n",
    "    # As above, this version is for the community model where embeddings are repeated.\n",
    "    \n",
    "        num_comms = embeddings.shape[0]\n",
    "        comms = self.comm_list_maker2(num_comms,n)\n",
    "        comm_probs = embeddings[:,0]\n",
    "        s_max = scipy.special.softmax(comm_probs)\n",
    "        \n",
    "        embeddings_length = embeddings.shape[1]\n",
    "        for t in range(len(comms)):\n",
    "            embeddings_subgraph = embeddings[comms[t],:] # assume each row is a vertex and full row is its embedding\n",
    "\n",
    "            del_P_del_z = np.zeros([embeddings.shape[0],embeddings.shape[1]])\n",
    "            for i in range(n): \n",
    "                # for each node, compute derivative of P wrt that node's embedding, multiply by derivative of embedding wrt weights, add up\n",
    "                total_i = np.zeros([embeddings_length-2])\n",
    "                for j in range(n):\n",
    "                    if i == j:\n",
    "                        continue\n",
    "\n",
    "                    #########################\n",
    "                    embed_i = embeddings_subgraph[i,2:]\n",
    "                    embed_j = embeddings_subgraph[j,2:]\n",
    "\n",
    "                    #col vector\n",
    "                    #for k in range(embeddings_length):\n",
    "                        #phi_ij_prime[k] = (self.hv(phi_ij)*embeddings_subgraph[j,k]*norm_i*norm_j - norm_j*embeddings_subgraph[i,k]*abs(phi_ij)/norm_i) / (norm_i**2 * norm_j**2) # calculated derivative\n",
    "                    phi_ij_prime = self.kern_deriv(embed_i, embed_j)\n",
    "                    #########################\n",
    "\n",
    "                    if adj[i,j] == 1:\n",
    "                        # chain rule\n",
    "                        total_i += phi_ij_prime * self.PLA_alt(adj,embeddings,comms[t],n,i,j,self.dropout)                   \n",
    "                    elif adj[i,j] == 0:\n",
    "                        total_i += -phi_ij_prime * self.PLA_alt(adj,embeddings,comms[t],n,i,j,self.dropout)\n",
    "                    \n",
    "                # derivatives wrt dropout probs\n",
    "                if drop:\n",
    "                    # can apply a scale factor here or elsewhere if needed\n",
    "                    removal_prob_deriv = self.PLA_alt2(adj,embeddings,node_selections,n,i) * self.sigm_deriv(embeddings_subgraph[i,1])\n",
    "                    total_i = total_i.ravel()\n",
    "                    total_i = np.insert(total_i, 0, [0,removal_prob_deriv])\n",
    "                else:\n",
    "                    total_i = total_i.ravel()\n",
    "                    total_i = np.insert(total_i, 0, [0,0])\n",
    "\n",
    "                #del_P_del_z[node_selections[i],:] += total_i # put the derivatives in the correct place for each node\n",
    "                del_P_del_z[comms[t][i],:] += total_i # put the derivatives in the correct place for each node\n",
    "            \n",
    "            # This part computes the derivatives wrt each of the community probs\n",
    "            num_repeats = np.zeros(num_comms)\n",
    "            comm_prob_deriv_factor = 1\n",
    "            for i in range(num_comms):\n",
    "                num_repeats[i] = comms[t].count(i)\n",
    "                comm_prob_deriv_factor = s_max[i] ** num_repeats[i]\n",
    "            for i in range(num_comms):\n",
    "                comm_prob_deriv = num_repeats[i] - n*s_max[i]\n",
    "                comm_prob_deriv = comm_prob_deriv * comm_prob_deriv_factor\n",
    "                comm_prob_deriv = comm_prob_deriv * self.PLA(adj, embeddings, comms[t], n, self.dropout)\n",
    "                del_P_del_z[i,0] += comm_prob_deriv\n",
    "                # might multiply derivative by a small number (.01 used previously) to make the steps smaller here\n",
    "                \n",
    "        #print(del_P_del_z[:,0])\n",
    "        del_P_del_z = del_P_del_z.ravel()\n",
    "        \n",
    "        return del_P_del_z\n",
    "    \n",
    "    def comm_list_maker2(self,r,s):\n",
    "        created_list = []\n",
    "        for i in range(r**s):\n",
    "            created_list.append(self.number_to_base(i,r,s))\n",
    "        return created_list\n",
    "        \n",
    "    def number_to_base(self,n,b,r):\n",
    "        # n = number, b = base, r = length of digits including added zeros\n",
    "        # currently returns numbers with ones digits first and zeros at the end\n",
    "        if n == 0:\n",
    "            return [0]*r\n",
    "        digits = []\n",
    "        while n:\n",
    "            digits.append(int(n % b))\n",
    "            n //= b\n",
    "        digits.extend([0]*(r-len(digits)))\n",
    "        return digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b187dea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard dot product kernel \n",
    "\n",
    "@jit\n",
    "def dotprod_kern(em1,em2):\n",
    "    return abs(np.dot(em1,em2))/(np.linalg.norm(em1)*np.linalg.norm(em2))\n",
    "\n",
    "def hv(x):\n",
    "    if x >= 0:\n",
    "        return 1\n",
    "    else: \n",
    "        return -1\n",
    "@jit\n",
    "def dotprod_kern_prime(em1,em2):\n",
    "    phi_ij = np.dot(em1,em2)\n",
    "    norm_i = np.linalg.norm(em1)\n",
    "    norm_j = np.linalg.norm(em2)\n",
    "    phi_ij_prime = (hv(phi_ij)*em2*norm_i*norm_j - norm_j*em1*abs(phi_ij)/norm_i) / (norm_i**2 * norm_j**2) # calculated derivative\n",
    "    return phi_ij_prime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e84a543d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# radial basis function kernel\n",
    "\n",
    "@jit\n",
    "def rbf_kern(em1, em2):\n",
    "    sig = 100\n",
    "    return exp(-(np.linalg.norm(em1 - em2)**2)/(2*sig**2))\n",
    "\n",
    "\n",
    "def rbf_kern_prime(em1, em2):\n",
    "    sig = 100 # may want to adjust this \n",
    "    rbf_prime = np.zeros([len(em1),1])\n",
    "    kern_val = exp(-(np.linalg.norm(em1 - em2)**2)/(2*sig**2))\n",
    "    rbf_prime = (-1/sig**2)*kern_val*(em1 - em2)\n",
    "    return rbf_prime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2627d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rbf kernel with scale factor\n",
    "\n",
    "#@jit\n",
    "def rbf2_kern(em1, em2):\n",
    "    # version of RBF kern with scale factor (1+t^2)^(-1/2) in front\n",
    "    sig = 10\n",
    "    scale = 2\n",
    "    return (1 + np.linalg.norm(em1)**2/scale)**(-1/2) * (1 + np.linalg.norm(em2)**2/scale)**(-1/2) * exp(-(np.linalg.norm(em1 - em2)**2)/(2*sig**2))\n",
    "\n",
    "#@jit\n",
    "def rbf2_kern_prime(em1, em2):\n",
    "    sig = 10\n",
    "    scale = 2\n",
    "    rbf_prime = np.zeros([len(em1),1])\n",
    "    kern_val_Fx = (1 + np.linalg.norm(em1)**2/2)**(-1/2)\n",
    "    kern_val_Fy = (1 + np.linalg.norm(em2)**2/2)**(-1/2)\n",
    "    kern_val_G = exp(-(np.linalg.norm(em1 - em2)**2)/(2*sig**2))\n",
    "    rbf_prime = kern_val_Fy * ( (-em1/scale*kern_val_Fx**3)*kern_val_G + (-1/sig**2)*kern_val_G*(em1 - em2)*kern_val_Fx )\n",
    "    return rbf_prime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6843ae11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def better_sampler(maxi, length, num_samples, no_replace=0):\n",
    "    y = np.arange(maxi)\n",
    "    x = np.zeros([num_samples,length])\n",
    "    if no_replace:\n",
    "        x = np.floor(maxi*np.random.rand(num_samples,length))\n",
    "    else:\n",
    "        for i in range(num_samples):\n",
    "            np.random.shuffle(y)\n",
    "            x[i,:] = y[0:length]\n",
    "    return x.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b84798e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a function which converts a graph to its graphlets via sampling\n",
    "\n",
    "#@jit\n",
    "def graph2graphlets(adj, max_order, graphlet_list_3, graphlet_list_4, graphlet_list_5, graphlet_list_6, counts_or_prop=0):\n",
    "    # converts a graph given in adj to a list of graphlet proportions via sampling\n",
    "\n",
    "    n = adj.shape[0]\n",
    "\n",
    "    graphlets_2 = np.zeros([2,1])\n",
    "    graphlets_3 = np.zeros([4,1])\n",
    "    graphlets_4 = np.zeros([11,1])\n",
    "    graphlets_5 = np.zeros([34,1])\n",
    "    graphlets_6 = np.zeros([156,1])\n",
    "\n",
    "\n",
    "    # Count the 2-graphlets ie. edges\n",
    "    #num_2 = n*(n-1)/2 \n",
    "    #graphlets_2[0] = (np.sum(adj)/2) / num_2 \n",
    "    #graphlets_2[1] = (num_2 - graphlets_2[0]) / num_2\n",
    "\n",
    "\n",
    "    # Count the 3-graphlets (via sampling)\n",
    "    num_3 = scipy.special.binom(n,3)\n",
    "    num_samples_3 = 100\n",
    "\n",
    "    nodes_all = better_sampler(n, 3, num_samples_3)\n",
    "\n",
    "    for i in range(num_samples_3):\n",
    "        # randomly select 3 nodes from the list, check what graph they form, check its equivalence class, add to count\n",
    "        nodes = nodes_all[i,:]\n",
    "        sub_adj = adj[nodes[:, None], nodes]\n",
    "        sub_num = graph2int(sub_adj)\n",
    "        graphlet_ID = graphlet_list_3[sub_num]\n",
    "        graphlets_3[graphlet_ID] += 1\n",
    "\n",
    "    graphlets_3 = graphlets_3 / num_samples_3\n",
    "    if counts_or_prop:\n",
    "        graphlets_3 = graphlets_3 * num_3\n",
    "\n",
    "    if max_order >= 4:\n",
    "        # Count the 4-graphlets (via sampling)\n",
    "        num_4 = scipy.special.binom(n,4)\n",
    "        num_samples_4 = 300\n",
    "        nodes_all = better_sampler(n, 4, num_samples_4)\n",
    "\n",
    "        for i in range(num_samples_4):\n",
    "            # randomly select 3 nodes from the list\n",
    "            nodes = nodes_all[i,:]\n",
    "            sub_adj = adj[nodes[:, None], nodes]\n",
    "            sub_num = graph2int(sub_adj)\n",
    "            graphlet_ID = graphlet_list_4[sub_num]\n",
    "            graphlets_4[graphlet_ID] += 1\n",
    "\n",
    "        graphlets_4 = graphlets_4 / num_samples_4\n",
    "        if counts_or_prop:\n",
    "            graphlets_4 = graphlets_4 * num_4\n",
    "\n",
    "    if max_order >= 5:\n",
    "        # Count the 5-graphlets (via sampling)\n",
    "        num_5 = scipy.special.binom(n,5)\n",
    "        num_samples_5 = 900\n",
    "        nodes_all = better_sampler(n, 5, num_samples_5)\n",
    "\n",
    "        for i in range(num_samples_5):\n",
    "            # randomly select 5 nodes from the list\n",
    "            nodes = nodes_all[i,:]\n",
    "            sub_adj = adj[nodes[:, None], nodes]\n",
    "            sub_num = graph2int(sub_adj)\n",
    "            graphlet_ID = graphlet_list_5[sub_num]\n",
    "            graphlets_5[graphlet_ID] += 1\n",
    "\n",
    "        graphlets_5 = graphlets_5 / num_samples_5\n",
    "        if counts_or_prop:\n",
    "            graphlets_5 = graphlets_5 * num_5\n",
    "\n",
    "    if max_order >= 6:\n",
    "        # Count the 6-graphlets (via sampling)\n",
    "        num_6 = scipy.special.binom(n,6)\n",
    "        num_samples_6 = 4500\n",
    "        nodes_all = better_sampler(n, 6, num_samples_6)\n",
    "\n",
    "        for i in range(num_samples_6):\n",
    "            # randomly select 6 nodes from the list\n",
    "            nodes = nodes_all[i,:]\n",
    "            sub_adj = adj[nodes[:, None], nodes]\n",
    "            sub_num = graph2int(sub_adj)\n",
    "            graphlet_ID = graphlet_list_6[sub_num]\n",
    "            graphlets_6[graphlet_ID] += 1\n",
    "\n",
    "        graphlets_6 = graphlets_6 / num_samples_6\n",
    "        if counts_or_prop:\n",
    "            graphlets_6 = graphlets_6 * num_6\n",
    "\n",
    "\n",
    "    # activate below line to use 2-graphlets also\n",
    "    #graphlets = np.concatenate((graphlets_2,graphlets_3,graphlets_4,graphlets_5,graphlets_6)).ravel() \n",
    "    graphlets = np.concatenate((graphlets_3,graphlets_4,graphlets_5,graphlets_6)).ravel()\n",
    "    return graphlets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05137bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def graph2int(adj):\n",
    "    # converts a graph whose adjacency matrix is adj into a number between 0 and 2^(n*(n-1)/2) - 1 using binary representation\n",
    "    # same function as above\n",
    "    \n",
    "    n = adj.shape[0]\n",
    "    bin_vector = np.array([adj[0,1]])\n",
    "    \n",
    "    for i in range(2,n):\n",
    "        appendage = adj[0:i,i]\n",
    "        bin_vector = np.append(bin_vector, appendage)\n",
    "    \n",
    "    out = 0\n",
    "    for i in range(bin_vector.size):\n",
    "        out += int(2**i * bin_vector[i])\n",
    "        \n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e64898e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def int2graphs(I, n):\n",
    "        # converts an integer I to an n x n graph\n",
    "\n",
    "        binary = \"{0:b}\".format(I)[::-1]\n",
    "\n",
    "        m = int(n*(n-1)/2)\n",
    "        num_zeros_to_add = m - len(binary)\n",
    "        zeros_to_add = \"0\"*num_zeros_to_add\n",
    "        binary = binary + zeros_to_add\n",
    "\n",
    "        graph = np.zeros([n,n])\n",
    "        count = 0\n",
    "        for j in range(1,n):\n",
    "            for i in range(j):\n",
    "                graph[i,j] = int(binary[count])\n",
    "                graph[j,i] = graph[i,j]\n",
    "                count += 1\n",
    "        return graph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f93c8ac",
   "metadata": {},
   "source": [
    "# Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71193f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AIDS dataset\n",
    "\n",
    "# load the data\n",
    "\n",
    "# There is a lot of preprocessing that happens here to get the data into a usable form\n",
    "\n",
    "A = []\n",
    "with open(\"aids_A.csv\") as csvfile:\n",
    "    reader = csv.reader(csvfile) # change contents to floats\n",
    "    for row in reader: # each row is a list\n",
    "        A.append(row)\n",
    "        \n",
    "A[0][0] = 1\n",
    "A = np.array(A)\n",
    "A = A.astype(np.int)\n",
    "\n",
    "\n",
    "graph_indicator = []\n",
    "with open(\"aids_indicator.csv\") as csvfile:\n",
    "    reader = csv.reader(csvfile) # change contents to floats\n",
    "    for row in reader: # each row is a list\n",
    "        graph_indicator.append(row)\n",
    "        \n",
    "graph_indicator[0][0] = 1\n",
    "graph_indicator = np.array(graph_indicator)\n",
    "graph_indicator = graph_indicator.astype(np.int)\n",
    "\n",
    "\n",
    "graph_labels = []\n",
    "with open(\"aids_labels.csv\") as csvfile:\n",
    "    reader = csv.reader(csvfile) # change contents to floats\n",
    "    for row in reader: # each row is a list\n",
    "        graph_labels.append(row)\n",
    "        \n",
    "graph_labels[0][0] = 0\n",
    "graph_labels = np.array(graph_labels)\n",
    "graph_labels = graph_labels.astype(np.int)\n",
    "\n",
    "#print(graph_labels)\n",
    "#with open('aids_indicator.csv', newline='') as csvfile:\n",
    "#    graph_indicator = list(csv.reader(csvfile))\n",
    "\n",
    "\n",
    "graphs = []\n",
    "\n",
    "start_points = [0]\n",
    "end_points = []\n",
    "for i in range(1,2001):\n",
    "    for j in range(start_points[i-1],31385):\n",
    "        if j == 31384:\n",
    "            end_points.append(j+2)\n",
    "            num_nodes = end_points[i-1] - start_points[i-1]\n",
    "            graphs.append(np.zeros([num_nodes,num_nodes]))\n",
    "        if graph_indicator[j,0] > i:\n",
    "            end_points.append(j+1)\n",
    "            #print(end_points)\n",
    "            if i != 2000:\n",
    "                start_points.append(j+1)\n",
    "            num_nodes = end_points[i-1] - start_points[i-1]\n",
    "            graphs.append(np.zeros([num_nodes,num_nodes]))\n",
    "            break\n",
    "            \n",
    "for i in range(64780):\n",
    "    point1 = A[i,0]\n",
    "    point2 = A[i,1]\n",
    "    for j in range(2000):\n",
    "        if end_points[j] >= point1 and end_points[j] >= point2:\n",
    "            graph_num = j\n",
    "            break\n",
    "    #print(start_points)\n",
    "    #print(point1,point2,graph_num,start_points[graph_num],end_points[graph_num])\n",
    "    \n",
    "    graphs[graph_num][point1-start_points[graph_num], point2-start_points[graph_num]] = 1\n",
    "    graphs[graph_num][point2-start_points[graph_num], point1-start_points[graph_num]] = 1\n",
    "    \n",
    "#shuffled = list(zip(graphs,graph_labels))\n",
    "#shuffle(shuffled)\n",
    "#graphs, graph_labels = zip(*shuffled)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75bfc137",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create Hbar for AIDS dataset.\n",
    "\n",
    "Hbar_avg_AIDS = 0\n",
    "j = 0\n",
    "for i in range(3000):\n",
    "    graph_number = np.random.randint(2000)\n",
    "    if graph_labels[graph_number,0] == 1:\n",
    "        continue\n",
    "    else:\n",
    "        Hbar = graph2graphlets(graphs[graph_number], 4, classes3, classes4, classes5, classes6)\n",
    "        j += 1\n",
    "        Hbar_avg_AIDS = Hbar_avg_AIDS*(j-1)/(j) + 1/(j)*Hbar\n",
    "\n",
    "print(Hbar_avg_AIDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20abd0bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# COX2_MD dataset\n",
    "\n",
    "# load the data\n",
    "\n",
    "# There is a lot of preprocessing that happens here to get the data into a usable form\n",
    "\n",
    "A = []\n",
    "with open(\"cox2_md_A.csv\") as csvfile:\n",
    "    reader = csv.reader(csvfile) # change contents to floats\n",
    "    for row in reader: # each row is a list\n",
    "        A.append(row)\n",
    "        \n",
    "A[0][0] = 1\n",
    "A = np.array(A)\n",
    "A = A.astype(np.int)\n",
    "\n",
    "\n",
    "graph_indicator = []\n",
    "with open(\"cox2_md_indicator.csv\") as csvfile:\n",
    "    reader = csv.reader(csvfile) # change contents to floats\n",
    "    for row in reader: # each row is a list\n",
    "        graph_indicator.append(row)\n",
    "        \n",
    "graph_indicator[0][0] = 1\n",
    "graph_indicator = np.array(graph_indicator)\n",
    "graph_indicator = graph_indicator.astype(np.int)\n",
    "\n",
    "\n",
    "graph_labels = []\n",
    "with open(\"cox2_md_labels.csv\") as csvfile:\n",
    "    reader = csv.reader(csvfile) # change contents to floats\n",
    "    for row in reader: # each row is a list\n",
    "        graph_labels.append(row)\n",
    "        \n",
    "graph_labels[0][0] = 0\n",
    "graph_labels = np.array(graph_labels)\n",
    "graph_labels = graph_labels.astype(np.int)\n",
    "\n",
    "#print(graph_labels)\n",
    "#with open('aids_indicator.csv', newline='') as csvfile:\n",
    "#    graph_indicator = list(csv.reader(csvfile))\n",
    "\n",
    "\n",
    "graphs = []\n",
    "\n",
    "start_points = [0]\n",
    "end_points = []\n",
    "for i in range(1,304): # one more than the number of graphs in the dataset\n",
    "    for j in range(start_points[i-1],graph_indicator.shape[0]): # number of nodes\n",
    "        if j == graph_indicator.shape[0]-1: # one fewer than the number of nodes ie. the number of rows in graph_indicator\n",
    "            end_points.append(j+2)\n",
    "            num_nodes = end_points[i-1] - start_points[i-1]\n",
    "            graphs.append(np.zeros([num_nodes,num_nodes]))\n",
    "        if graph_indicator[j,0] > i:\n",
    "            end_points.append(j+1)\n",
    "            #print(end_points)\n",
    "            if i != 303:\n",
    "                start_points.append(j+1)\n",
    "            num_nodes = end_points[i-1] - start_points[i-1]\n",
    "            graphs.append(np.zeros([num_nodes,num_nodes]))\n",
    "            break\n",
    "            \n",
    "for i in range(A.shape[0]): # the number of edges, ie. the number of rows in A\n",
    "    point1 = A[i,0]\n",
    "    point2 = A[i,1]\n",
    "    for j in range(303):\n",
    "        if end_points[j] >= point1 and end_points[j] >= point2:\n",
    "            graph_num = j\n",
    "            break\n",
    "    #print(start_points)\n",
    "    #print(point1,point2,graph_num,start_points[graph_num],end_points[graph_num])\n",
    "    \n",
    "    graphs[graph_num][point1-start_points[graph_num], point2-start_points[graph_num]] = 1\n",
    "    graphs[graph_num][point2-start_points[graph_num], point1-start_points[graph_num]] = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44ca723b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create Hbar for COX2_MD dataset.\n",
    "\n",
    "Hbar_avg_COX = 0\n",
    "j = 0\n",
    "for i in range(3000): # does not depend on data\n",
    "    graph_number = np.random.randint(303) # number of graphs\n",
    "    if graph_labels[graph_number,0] == 1:\n",
    "        continue\n",
    "    else:\n",
    "        Hbar = graph2graphlets(graphs[graph_number],4, classes3, classes4, classes5, classes6)\n",
    "        j += 1\n",
    "        Hbar_avg_COX = Hbar_avg_COX*(j-1)/(j) + 1/(j)*Hbar\n",
    "\n",
    "print(Hbar_avg_COX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe60fd28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OHSU dataset\n",
    "\n",
    "# load the data\n",
    "\n",
    "# There is a lot of preprocessing that happens here to get the data into a usable form\n",
    "\n",
    "A = []\n",
    "with open(\"ohsu_A.csv\") as csvfile:\n",
    "    reader = csv.reader(csvfile) # change contents to floats\n",
    "    for row in reader: # each row is a list\n",
    "        A.append(row)\n",
    "        \n",
    "A[0][0] = 1\n",
    "A = np.array(A)\n",
    "A = A.astype(np.int)\n",
    "\n",
    "\n",
    "graph_indicator = []\n",
    "with open(\"ohsu_indicator.csv\") as csvfile:\n",
    "    reader = csv.reader(csvfile) # change contents to floats\n",
    "    for row in reader: # each row is a list\n",
    "        graph_indicator.append(row)\n",
    "        \n",
    "graph_indicator[0][0] = 1\n",
    "graph_indicator = np.array(graph_indicator)\n",
    "graph_indicator = graph_indicator.astype(np.int)\n",
    "\n",
    "\n",
    "graph_labels = []\n",
    "with open(\"ohsu_labels.csv\") as csvfile:\n",
    "    reader = csv.reader(csvfile) # change contents to floats\n",
    "    for row in reader: # each row is a list\n",
    "        graph_labels.append(row)\n",
    "        \n",
    "graph_labels[0][0] = 0\n",
    "graph_labels = np.array(graph_labels)\n",
    "graph_labels = graph_labels.astype(np.int)\n",
    "\n",
    "#print(graph_labels)\n",
    "#with open('aids_indicator.csv', newline='') as csvfile:\n",
    "#    graph_indicator = list(csv.reader(csvfile))\n",
    "\n",
    "\n",
    "graphs = []\n",
    "\n",
    "start_points = [0]\n",
    "end_points = []\n",
    "for i in range(1,80): # one more than the number of graphs in the dataset\n",
    "    for j in range(start_points[i-1],graph_indicator.shape[0]): # number of edges\n",
    "        if j == graph_indicator.shape[0]-1: # one fewer than the number of edges ie. the number of rows in graph_indicator\n",
    "            end_points.append(j+2)\n",
    "            num_nodes = end_points[i-1] - start_points[i-1]\n",
    "            graphs.append(np.zeros([num_nodes,num_nodes]))\n",
    "        if graph_indicator[j,0] > i:\n",
    "            end_points.append(j+1)\n",
    "            #print(end_points)\n",
    "            if i != 79: # number of graphs\n",
    "                start_points.append(j+1)\n",
    "            num_nodes = end_points[i-1] - start_points[i-1]\n",
    "            graphs.append(np.zeros([num_nodes,num_nodes]))\n",
    "            break\n",
    "            \n",
    "for i in range(A.shape[0]): # the number of edges, ie. the number of rows in A\n",
    "    point1 = A[i,0]\n",
    "    point2 = A[i,1]\n",
    "    for j in range(79): # number of graphs\n",
    "        if end_points[j] >= point1 and end_points[j] >= point2:\n",
    "            graph_num = j\n",
    "            break\n",
    "    #print(start_points)\n",
    "    #print(point1,point2,graph_num,start_points[graph_num],end_points[graph_num])\n",
    "    \n",
    "    graphs[graph_num][point1-start_points[graph_num], point2-start_points[graph_num]] = 1\n",
    "    graphs[graph_num][point2-start_points[graph_num], point1-start_points[graph_num]] = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aee582c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create Hbar for OHSU dataset.\n",
    "\n",
    "Hbar_avg_OHSU = 0\n",
    "j = 0\n",
    "for i in range(1000): # does not depend on data\n",
    "    graph_number = np.random.randint(79) # number of graphs\n",
    "    if graph_labels[graph_number,0] == 1:\n",
    "        continue\n",
    "    else:\n",
    "        Hbar = graph2graphlets(graphs[graph_number], classes3, classes4, classes5, classes6)\n",
    "        j += 1\n",
    "        Hbar_avg_OHSU = Hbar_avg_OHSU*(j-1)/(j) + 1/(j)*Hbar\n",
    "\n",
    "print(Hbar_avg_OHSU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed0289ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Proteins dataset\n",
    "\n",
    "# load the data\n",
    "\n",
    "# There is a lot of preprocessing that happens here to get the data into a usable form\n",
    "\n",
    "A = []\n",
    "with open(\"proteins_A.csv\") as csvfile:\n",
    "    reader = csv.reader(csvfile) # change contents to floats\n",
    "    for row in reader: # each row is a list\n",
    "        A.append(row)\n",
    "        \n",
    "A[0][0] = 12\n",
    "A = np.array(A)\n",
    "A = A.astype(np.int)\n",
    "\n",
    "\n",
    "graph_indicator = []\n",
    "with open(\"proteins_indicator.csv\") as csvfile:\n",
    "    reader = csv.reader(csvfile) # change contents to floats\n",
    "    for row in reader: # each row is a list\n",
    "        graph_indicator.append(row)\n",
    "        \n",
    "graph_indicator[0][0] = 1\n",
    "graph_indicator = np.array(graph_indicator)\n",
    "graph_indicator = graph_indicator.astype(np.int)\n",
    "\n",
    "\n",
    "graph_labels = []\n",
    "with open(\"proteins_labels.csv\") as csvfile:\n",
    "    reader = csv.reader(csvfile) # change contents to floats\n",
    "    for row in reader: # each row is a list\n",
    "        graph_labels.append(row)\n",
    "        \n",
    "graph_labels[0][0] = 0\n",
    "graph_labels = np.array(graph_labels)\n",
    "graph_labels = graph_labels.astype(np.int)\n",
    "\n",
    "\n",
    "\n",
    "number_graphs = 1113\n",
    "number_nodes = graph_indicator.shape[0] #43471\n",
    "number_edges = 162088\n",
    "\n",
    "graphs = []\n",
    "\n",
    "start_points = [0]\n",
    "end_points = []\n",
    "for i in range(1,number_graphs+1):\n",
    "    for j in range(start_points[i-1],number_nodes):\n",
    "        if j == number_nodes-1:\n",
    "            end_points.append(j+2)\n",
    "            num_nodes = end_points[i-1] - start_points[i-1]\n",
    "            graphs.append(np.zeros([num_nodes,num_nodes]))\n",
    "        if graph_indicator[j,0] > i:\n",
    "            end_points.append(j+1)\n",
    "            #print(end_points)\n",
    "            if i != number_graphs:\n",
    "                start_points.append(j+1)\n",
    "            num_nodes = end_points[i-1] - start_points[i-1]\n",
    "            graphs.append(np.zeros([num_nodes,num_nodes]))\n",
    "            break\n",
    "                        \n",
    "for i in range(number_edges):\n",
    "    point1 = A[i,0]\n",
    "    point2 = A[i,1]\n",
    "    for j in range(number_graphs):\n",
    "        if end_points[j] >= point1 and end_points[j] >= point2:\n",
    "            graph_num = j\n",
    "            break\n",
    "    #print(start_points)\n",
    "    #print(point1,point2,graph_num,start_points[graph_num],end_points[graph_num])\n",
    "    \n",
    "    graphs[graph_num][point1-start_points[graph_num], point2-start_points[graph_num]] = 1\n",
    "    graphs[graph_num][point2-start_points[graph_num], point1-start_points[graph_num]] = 1\n",
    "    \n",
    "#shuffled = list(zip(graphs,graph_labels))\n",
    "#shuffle(shuffled)\n",
    "#graphs, graph_labels = zip(*shuffled)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b93662e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "Hbar_avg_proteins = 0\n",
    "j = 0\n",
    "for i in range(3000):\n",
    "    graph_number = np.random.randint(number_graphs)\n",
    "    if graph_labels[graph_number,0] == 1 or graphs[graph_number].shape[0] <= 4:\n",
    "        continue\n",
    "    else:\n",
    "        Hbar = graph2graphlets(graphs[graph_number], 4, classes3, classes4, classes5, classes6)\n",
    "        j += 1\n",
    "        Hbar_avg_proteins = Hbar_avg_proteins*(j-1)/(j) + 1/(j)*Hbar\n",
    "\n",
    "print(Hbar_avg_proteins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eed42af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Brain dataset\n",
    "# 114 graphs, 70 nodes each\n",
    "\n",
    "graphs = []\n",
    "\n",
    "for i in range(114):\n",
    "    A = []\n",
    "    filename = \"carey_data/carey_data_\" + str(i) + \".csv\"\n",
    "    with open(filename) as csvfile:\n",
    "        reader = csv.reader(csvfile) # change contents to floats\n",
    "        for row in reader: # each row is a list\n",
    "            A.append(row)\n",
    "    A = np.array(A)\n",
    "    A = A[1:,1:]\n",
    "    A = A.astype(np.int)\n",
    "    graphs.append(A)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75bfa0b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create Hbar for Brain dataset.\n",
    "\n",
    "Hbar_avg_carey = 0\n",
    "j = 0\n",
    "for i in range(1000): # does not depend on data\n",
    "    graph_number = 2*np.random.randint(57) # number of graphs\n",
    "    Hbar = graph2graphlets(graphs[graph_number],4, classes3, classes4, classes5, classes6)\n",
    "    j += 1\n",
    "    Hbar_avg_carey = Hbar_avg_carey*(j-1)/(j) + 1/(j)*Hbar\n",
    "\n",
    "print(Hbar_avg_carey)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82274723",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the empty graph to try to learn\n",
    "\n",
    "adj3 = np.zeros([10,10])\n",
    "Hbar_avg3 = graph2graphlets(adj3, classes3, classes4, classes5, classes6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e99485da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A simple SBM to try to learn\n",
    "\n",
    "group_probs = np.array([.25,.25,.25,.25]) # block membership probs\n",
    "edge_probs = np.array([[.75,.1,.1,.1],[.1,.75,.1,.1],[.1,.1,.75,.1],[.1,.1,.1,.75]]) # edge probs by block\n",
    "\n",
    "num_nodes_sbm = 16\n",
    "\n",
    "H_sbm = 0\n",
    "for i in range(200):\n",
    "    assignments = np.random.randint(low=0,high=4,size=num_nodes_sbm)\n",
    "    adj_sbm = np.zeros([num_nodes_sbm,num_nodes_sbm])\n",
    "    for j in range(num_nodes_sbm):\n",
    "        for k in range(j+1,num_nodes_sbm):\n",
    "            edge_prob = edge_probs[assignments[j]][assignments[k]]\n",
    "            rand = uniform(0, 1)\n",
    "            if rand < edge_prob:\n",
    "                adj_sbm[j,k] = 1\n",
    "                adj_sbm[k,j] = 1\n",
    "    H_sbm_iter = graph2graphlets(adj_sbm,4,classes3,classes4,classes5,classes6)\n",
    "    H_sbm += (1/200)*H_sbm_iter\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7e99aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The \"Community\" graph from the GraphRNN paper\n",
    "\n",
    "group_probs = np.array([.5,.5]) # block membership probs\n",
    "edge_probs = np.array([[.3,.05],[.05,.3]]) # edge probs by block\n",
    "#edge_probs = np.array([[1.0,.0],[.0,1.0]])\n",
    "\n",
    "num_nodes_sbm = 80\n",
    "num_graphs_sbm = 500\n",
    "\n",
    "H_sbm = 0\n",
    "for i in range(num_graphs_sbm):\n",
    "    assignments = np.random.randint(low=0,high=2,size=num_nodes_sbm)\n",
    "    adj_sbm = np.zeros([num_nodes_sbm,num_nodes_sbm])\n",
    "    for j in range(num_nodes_sbm):\n",
    "        for k in range(j+1,num_nodes_sbm):\n",
    "            edge_prob = edge_probs[assignments[j]][assignments[k]]\n",
    "            rand = uniform(0, 1)\n",
    "            if rand < edge_prob:\n",
    "                adj_sbm[j,k] = 1\n",
    "                adj_sbm[k,j] = 1\n",
    "    H_sbm_iter = graph2graphlets(adj_sbm,4,classes3,classes4,classes5,classes6)\n",
    "    H_sbm += (1/num_graphs_sbm)*H_sbm_iter\n",
    "    \n",
    "lims3 = np.argpartition(H_sbm[0:4], -2)[-2:]\n",
    "lims4 = np.argpartition(H_sbm[4:15], -6)[-6:]\n",
    "lims5 = np.argpartition(H_sbm[15:], -10)[-10:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8585c487",
   "metadata": {},
   "source": [
    "# Run Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9707f663",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and train an instance of the neural net\n",
    "\n",
    "device = th.device(\"cuda:0\" if th.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "#max_nodes = 2 # 16 is average nodes for AIDS dataset\n",
    "max_nodes = 10\n",
    "num_features = 8 # dimension of unit ball. depending on kernel, needs to be bigger than the number of nodes or some dists will be impossible\n",
    "max_nodes_multiple = 10000 # the actual number of nodes needed when using multi functionality\n",
    "minibatch = 100\n",
    "order = 4\n",
    "limited = 0 \n",
    "\n",
    "\n",
    "net_generate2 = graph_learner(H_sbm, max_nodes, num_features, rbf_kern, rbf_kern_prime)\n",
    "# optional arguments copied for convenience\n",
    "# multi=0, multi_probs=[], max_nodes_multi=0, dropout=0)\n",
    "\n",
    "net_generate2.to(device)\n",
    "\n",
    "max_epoch = 20000 #200\n",
    "gamma_start = 1\n",
    "gammas = gamma_start\n",
    "\n",
    "stop_threshold = .2 # total deviation from target distribution\n",
    "\n",
    "result = np.array([100])\n",
    "flag1 = 0\n",
    "flag2 = 0\n",
    "\n",
    "for epoch in range(max_epoch):\n",
    "    print(epoch)\n",
    "\n",
    "    if result[-1] < .5 and flag1 == 0:\n",
    "        gammas = gammas/10\n",
    "        flag1 = 1\n",
    "    if result[-1] < .2 and flag2 == 0 and flag1 == 1:\n",
    "        gammas = gammas/2\n",
    "        flag2 = 1\n",
    "        \n",
    "    if epoch % 10 == 0:\n",
    "        print(epoch)\n",
    "        net_out = net_generate2.train(minibatch, gammas, 1, 1, order, [.5,.5,0,0,0], limited)\n",
    "        result = np.append(result,net_out)\n",
    "    else:\n",
    "        net_out = net_generate2.train(minibatch, gammas, 0, 1, order, [.5,.5,0,0,0], limited)\n",
    "    \n",
    "    if net_out < stop_threshold:\n",
    "        print(epoch)\n",
    "        print(net_out)\n",
    "        result = np.append(result,net_out)\n",
    "        break\n",
    "\n",
    "# negative result = too few graphlets of that type\n",
    "# positive = too many of that type"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
